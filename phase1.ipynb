{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1\n",
    "# Aaryan Bhagat\n",
    "# 862468325"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold, StratifiedShuffleSplit, cross_val_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1      2      3       4       5        6        7        8        9   \\\n",
      "0    M  17.99  10.38  122.80  1001.0  0.11840  0.27760  0.30010  0.14710   \n",
      "1    M  20.57  17.77  132.90  1326.0  0.08474  0.07864  0.08690  0.07017   \n",
      "2    M  19.69  21.25  130.00  1203.0  0.10960  0.15990  0.19740  0.12790   \n",
      "3    M  11.42  20.38   77.58   386.1  0.14250  0.28390  0.24140  0.10520   \n",
      "4    M  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.19800  0.10430   \n",
      "..  ..    ...    ...     ...     ...      ...      ...      ...      ...   \n",
      "564  M  21.56  22.39  142.00  1479.0  0.11100  0.11590  0.24390  0.13890   \n",
      "565  M  20.13  28.25  131.20  1261.0  0.09780  0.10340  0.14400  0.09791   \n",
      "566  M  16.60  28.08  108.30   858.1  0.08455  0.10230  0.09251  0.05302   \n",
      "567  M  20.60  29.33  140.10  1265.0  0.11780  0.27700  0.35140  0.15200   \n",
      "568  B   7.76  24.54   47.92   181.0  0.05263  0.04362  0.00000  0.00000   \n",
      "\n",
      "         10  ...      22     23      24      25       26       27      28  \\\n",
      "0    0.2419  ...  25.380  17.33  184.60  2019.0  0.16220  0.66560  0.7119   \n",
      "1    0.1812  ...  24.990  23.41  158.80  1956.0  0.12380  0.18660  0.2416   \n",
      "2    0.2069  ...  23.570  25.53  152.50  1709.0  0.14440  0.42450  0.4504   \n",
      "3    0.2597  ...  14.910  26.50   98.87   567.7  0.20980  0.86630  0.6869   \n",
      "4    0.1809  ...  22.540  16.67  152.20  1575.0  0.13740  0.20500  0.4000   \n",
      "..      ...  ...     ...    ...     ...     ...      ...      ...     ...   \n",
      "564  0.1726  ...  25.450  26.40  166.10  2027.0  0.14100  0.21130  0.4107   \n",
      "565  0.1752  ...  23.690  38.25  155.00  1731.0  0.11660  0.19220  0.3215   \n",
      "566  0.1590  ...  18.980  34.12  126.70  1124.0  0.11390  0.30940  0.3403   \n",
      "567  0.2397  ...  25.740  39.42  184.60  1821.0  0.16500  0.86810  0.9387   \n",
      "568  0.1587  ...   9.456  30.37   59.16   268.6  0.08996  0.06444  0.0000   \n",
      "\n",
      "         29      30       31  \n",
      "0    0.2654  0.4601  0.11890  \n",
      "1    0.1860  0.2750  0.08902  \n",
      "2    0.2430  0.3613  0.08758  \n",
      "3    0.2575  0.6638  0.17300  \n",
      "4    0.1625  0.2364  0.07678  \n",
      "..      ...     ...      ...  \n",
      "564  0.2216  0.2060  0.07115  \n",
      "565  0.1628  0.2572  0.06637  \n",
      "566  0.1418  0.2218  0.07820  \n",
      "567  0.2650  0.4087  0.12400  \n",
      "568  0.0000  0.2871  0.07039  \n",
      "\n",
      "[569 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare Data\n",
    "\n",
    "df = pd.read_csv(\"breast_cancer_folder/wdbc.data\", header=None)\n",
    "# print(df.shape)\n",
    "# df.columns = [\"ID\", \"Diagnosis\", \"radius\", \"texture\", \"perimeter\", \"area\", \"smoothness\", \"compactness\", \"concavity\", \"concave points\", \"symmetry\", \"fractal dimension\"]\n",
    "new_df = df.drop(columns = df.columns[0], axis=1)\n",
    "# print(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "train_df, test_df = train_test_split(df, test_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, split_column, split_value):\n",
    "    \n",
    "    split_column_values = data[:, split_column]\n",
    "\n",
    "    data_below = data[split_column_values <= split_value]\n",
    "    data_above = data[split_column_values >  split_value]\n",
    "    \n",
    "    return data_below, data_above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uni_classes(df):\n",
    "    \n",
    "    label_column = df[:, -1]\n",
    "    unique_classes, counts_unique_classes = np.unique(label_column, return_counts=True)\n",
    "\n",
    "    index = counts_unique_classes.argmax()\n",
    "    classification = unique_classes[index]\n",
    "    \n",
    "    return classification\n",
    "\n",
    "def calculate_entropy(data):\n",
    "    \n",
    "    label_column = data[:, -1]\n",
    "    _, counts = np.unique(label_column, return_counts=True)\n",
    "\n",
    "    probabilities = counts / counts.sum()\n",
    "    entropy = sum(probabilities * -np.log2(probabilities))\n",
    "     \n",
    "    return entropy\n",
    "\n",
    "def calculate_overall_entropy(data_below, data_above):\n",
    "    \n",
    "    n = len(data_below) + len(data_above)\n",
    "    p_data_below = len(data_below) / n\n",
    "    p_data_above = len(data_above) / n\n",
    "\n",
    "    overall_entropy =  (p_data_below * calculate_entropy(data_below) \n",
    "                      + p_data_above * calculate_entropy(data_above))\n",
    "    \n",
    "    return overall_entropy\n",
    "\n",
    "def determine_best_split(data, potential_splits):\n",
    "    \n",
    "    overall_entropy = 9999\n",
    "    for column_index in potential_splits:\n",
    "        for value in potential_splits[column_index]:\n",
    "            data_below, data_above = split_data(data, split_column=column_index, split_value=value)\n",
    "            current_overall_entropy = calculate_overall_entropy(data_below, data_above)\n",
    "\n",
    "            if current_overall_entropy <= overall_entropy:\n",
    "                overall_entropy = current_overall_entropy\n",
    "                best_split_column = column_index\n",
    "                best_split_value = value\n",
    "    \n",
    "    return best_split_column, best_split_value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "heir_google",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
